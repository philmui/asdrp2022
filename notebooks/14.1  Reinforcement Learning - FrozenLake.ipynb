{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173a8683",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Frozenlake\n",
    "\n",
    "Before diving into the topic of **Deep Reinforcement** it is important to get to know the basic concepts of Reinforcement Learning as this allows us to improve our understanding of the whole field.\n",
    "\n",
    "To do so we take a look at one of the most known classical Reinforcement Learning algorithm called **Q-Learning** and apply this algorithm on the [FrozenLake environment](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf8f0d",
   "metadata": {},
   "source": [
    "<img src=\"./images/frozenlake.gif\" width=\"200px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1566a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad5b97",
   "metadata": {},
   "source": [
    "`is_slippery`: True/False. If True will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions.\n",
    "\n",
    "For example, if action is left and is_slippery is True, then:\n",
    "- P(move left)=1/3\n",
    "- P(move up)=1/3\n",
    "- P(move down)=1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # Reset to initial state\n",
    "for _ in range(5):\n",
    "    env.render()  # Render on the screen\n",
    "    action = env.action_space.sample()  # chose a random action\n",
    "    env.step(action)  # Perform random action on the environment\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb3fc",
   "metadata": {},
   "source": [
    "## Seeing ASCII movement  with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "for letter in ['a','b','c','d','e']:\n",
    "    print(letter)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712a6a0",
   "metadata": {},
   "source": [
    "## ASCII Visualization of FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b407049",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()  # Reset to initial state\n",
    "for epsiode in range(15):\n",
    "    print(observation)\n",
    "    env.render()  # Render on the screen\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "    action = env.action_space.sample()  # chose a random action\n",
    "    observation, reward, done, info = env.step(action)  # Perform random action on the environment\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c47851",
   "metadata": {},
   "source": [
    "## FrozenLake: Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06609917",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22774dc",
   "metadata": {},
   "source": [
    "The agent at each tile has one of 4 actions:\n",
    "- 0: LEFT\n",
    "- 1: DOWN\n",
    "- 2: RIGHT\n",
    "- 3: UP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50141891",
   "metadata": {},
   "source": [
    "## FrozenLake: Observation Space\n",
    "\n",
    "The observation is a value representing the agentâ€™s current position as `current_row * nrows + current_col` (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. \n",
    "\n",
    "The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations.  An observation is a single digit representing the current location, for a 4by4 \"lake\":\n",
    "\n",
    "     0   1   2   3\n",
    "     4   5   6   7\n",
    "     8   9  10  11\n",
    "    12  13  14  15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa187bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b1c07",
   "metadata": {},
   "source": [
    "# FrozenLake: Reward\n",
    "\n",
    "The agent is given a reward depending on whether it arrived at the \"goal\" tile or not:\n",
    "- Reached Goal (G): +1\n",
    "- Reached Hole (H): 0\n",
    "- Reached Frozen (F): 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0290987",
   "metadata": {},
   "source": [
    "## Manual Playing FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asdw():\n",
    "    '''\n",
    "    This function gets the key press for gym action choice\n",
    "    '''\n",
    "    k = input()\n",
    "    if k == 'a':\n",
    "        action = 0\n",
    "    if k == 's':\n",
    "        action = 1\n",
    "    if k == 'd':\n",
    "        action = 2\n",
    "    if k == 'w':\n",
    "        action = 3\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False)\n",
    "env.reset()  # Reset to initial state\n",
    "done = False\n",
    "print(f\"Press: a=left,w=up,s=down,d=right\")\n",
    "for _ in range(10):\n",
    "    if not done:\n",
    "        env.render()  # Render on the screen\n",
    "        clear_output(wait=True)\n",
    "        action = asdw()  # chose an action\n",
    "        observation, reward, done, info = env.step(action)  # Perform random action on the environment\n",
    "        print(f\"state={observation}, reward={reward}, done={done}\")\n",
    "env.close()  # dont forget to close the environment\n",
    "print(f\"Game over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e36e98",
   "metadata": {},
   "source": [
    "## Q-Table : valuing each step of the agent in the environment\n",
    "\n",
    "A \"Q-Table\" is essentially a mapping of all possible `state,action` pairs and the expected reward for taking an action at a particular state that we will keep updating.\n",
    "\n",
    "$$Q(s_t,a_t)$$\n",
    "\n",
    "For our simple discrete Frozen Lake problem, this means we have 4 actions for columns, and 16 possible states (player location on the 4 by 4 grid). So our table will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610c26a",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "    <th>A0 - LEFT</th>\n",
    "    <th>A1 - DOWN</th>\n",
    "    <th>A2 - RIGHT</th>\n",
    "    <th>A3 - UP</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>State 0</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td><strong>State 1</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "      <td><strong>State ...</strong></td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "        <td>...</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "      <td><strong>State 15</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "        <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeed638",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's initialize our Q-table of values with no prior knowledge about the game:\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12891402",
   "metadata": {},
   "source": [
    "## Learning the Q-Table\n",
    "\n",
    "The Q-Learning update functions will require hyperparameters. we'll define them here. Often the best place to choose a good starting value is reading publications or through experimentation. Unfortunately, its very difficult to give general advice, as most environments are radically different to each other, and often hyperparameter tuning is required.\n",
    "\n",
    "Q-Learning Equation Parameters: [Wikipedia](https://en.wikipedia.org/wiki/Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=20000       # number of epochs/episodes to train for\n",
    "ALPHA = 0.8        # the learning rate\n",
    "GAMMA = 0.95       # the discount rate\n",
    "MAX_EPISODES = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bc32a",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Learning : exploring vs exploitation\n",
    "\n",
    "We need to tune how quickly we \"explore\" the parameter space vs how quickly we \"exploit\" what we have learned to get to the goal.\n",
    "- A random number of times ($0.0 \\le \\epsilon \\le 1.0$), we want to randomly choose a move.\n",
    "- The other times with probability $(1 - \\epsilon)$, we choose the best action for a given state (row)\n",
    "\n",
    "Tuning $\\epsilon$ over time is a hyperparameter that we need to decide on. Reduce too fast, agent won't have enough time to learn. Reduce too slow, you're wasting time picking random actions. Key here is that these value help balance exploration (random choice) versus explotation (always picking what works for that Q(s,a). It's a tough balance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration vs. Exploitation parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff8481",
   "metadata": {},
   "source": [
    "## Q-Table Update (aka Learning)\n",
    "\n",
    "Now it is time to dive into the training / Q-Table update methodology.\n",
    "First we will define some functions needed for training phase\n",
    "\n",
    "- `epsilon_greedy_action_selection`: Is used to implement the epsilon greedy action selection routine.\n",
    "- `compute_next_q_value`: Computes the next Q-Values according to the formula from the lecture\n",
    "- `reduce_epsilon`: Reduces the  ðœ–  used for the epsilon greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c1706",
   "metadata": {},
   "source": [
    "### Selecting an Action with $\\epsilon$-greedy Strategy\n",
    "\n",
    "If we simply always select the `argmax()` qtable value during training, we'll most likely get stuck in an exploitation loop, so we'll use a random value to randomly select an action from time to time, helping the model explore , rather than exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca831d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    '''\n",
    "    Returns an action for the agent. Note how it uses a random number to decide on\n",
    "    exploration versus explotation trade-off.\n",
    "    '''\n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        # Action row for a particular state\n",
    "        state_row = q_table[discrete_state,:]\n",
    "        # Index of highest action for state\n",
    "        # Recall action is mapped to index (e.g. 0=LEFT, 1=DOWN, etc..)\n",
    "        action = np.argmax(state_row)\n",
    "    \n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        # Return a random 0,1,2,3 action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65320b",
   "metadata": {},
   "source": [
    "### Computing Q-Value\n",
    "\n",
    "Here we have our main Q-Learning update equation, note how it takes in the old q-value, the next optimal q value, along with our current reward, and then updates the next q value accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace43b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e664ad6",
   "metadata": {},
   "source": [
    "### Reducing Epsilon\n",
    "\n",
    "As training continues, we need to balance explotation versus exploration, we want ot make sure our agent doesn't get trapped in a cycle going from an F square to another F Square back and forth. We also don't want our agent permanently choosing random values. We'll use the function below to try to balance this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon,epoch):\n",
    "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d3338",
   "metadata": {},
   "source": [
    "### Let's TRAIN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([state_size, action_size])\n",
    "total_reward = 0\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9726e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the rewards to see progress later\n",
    "rewards = []\n",
    "for episode in range(EPOCHS):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(epsilon,q_table, state)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Look up current/old qtable value Q(s_t,a_t)\n",
    "        old_q_value =  q_table[state,action]  \n",
    "\n",
    "        # Get the next optimal Q-Value\n",
    "        next_optimal_q_value = np.max(q_table[new_state, :])  \n",
    "\n",
    "        # Compute next q value\n",
    "        next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)   \n",
    "\n",
    "        # Update Q Table\n",
    "        q_table[state,action] = next_q\n",
    "        \n",
    "        total_rewards = total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = reduce_epsilon(epsilon,episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb846cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCHS),np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85255e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e2816",
   "metadata": {},
   "source": [
    "## Playing with Learned Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "for _ in range(10):    \n",
    "    if not done:\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state])  # and chose action from the Q-Table\n",
    "        state, reward, done, info = env.step(action) # Finally perform the action\n",
    "\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "env.close()\n",
    "print(f\"Game over\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
